# 客户画像

客户画像需要坚持三个原则，分别是人口属性和信用信息为主，强相关信息为主，定性数据为主。

客户画像目的有两个：

- 业务场景出发，寻找目标客户；

- 参考用户画像的信息，为用户设计产品或开展营销活动。

参考：

[从相亲说起：从对公客户画像、贡献积分到智慧银行对公营销](https://zhuanlan.zhihu.com/p/54657038)

[金融行业大数据用户画像实践](https://www.woshipm.com/data-analysis/445963.html)

# 矩阵相关操作和算法

## SVD（奇异值分解）

**SVD（奇异值分解）**是一种**矩阵分解**的方法，用于**将一个矩阵分解为三个矩阵的乘积**。它在很多领域中都有广泛的应用，包括数据降维、推荐系统、图像处理等。

SVD将一个矩阵分解为以下三个矩阵的乘积：

- **U矩阵**：包含原始矩阵的左奇异向量（left singular vectors）组成的矩阵。它的列向量是正交的，并且是原始矩阵的特征向量。
- **Σ矩阵**：包含奇异值（singular values）组成的对角矩阵。奇异值是原始矩阵的特征值的平方根，表示了原始矩阵在每个特征向量方向上的重要性。
- **V矩阵**：包含原始矩阵的右奇异向量（right singular vectors）组成的矩阵。它的列向量也是正交的，并且是原始矩阵的特征向量。

SVD的数学表示为：A = UΣV^T，其中A是原始矩阵，U、Σ和V分别是分解后的三个矩阵。

通过SVD分解，我们可以对原始矩阵进行降维，只保留奇异值较大的特征向量，从而减少数据的维度。这在处理大规模数据和矩阵压缩方面非常有用。此外，SVD还可以用于推荐系统中的协同过滤算法，通过分解用户-物品评分矩阵，可以进行用户和物品的推荐。

需要注意的是，SVD的计算成本较高，特别是对于大型矩阵。因此，在实际应用中，通常会使用近似的SVD方法，如**截断SVD（Truncated SVD）**或 **随机SVD（Randomized SVD）**，以加快计算速度并降低内存消耗。

>特征值和特征向量是线性代数中的重要概念，用于描述矩阵的性质和变换。
>
>特征值（eigenvalue）是一个标量，表示线性变换中的一个重要特性。对于一个n维矩阵A，其特征值是满足方程 Av = λv 的标量λ，其中v是一个非零向量。换句话说，特征值λ表示了在线性变换下，向量v的方向保持不变，只发生了缩放的程度。
>
>特征向量（eigenvector）是与特征值相关联的向量。对于一个n维矩阵A和其特征值λ，特征向量v是满足方程 Av = λv 的非零向量。特征向量表示在线性变换下，保持其方向不变的向量。
>
>特征值和特征向量的重要性在于它们提供了矩阵的基本性质和变换的特征。通过计算矩阵的特征值和特征向量，我们可以了解矩阵的对称性、稳定性、奇异性等信息。在实际应用中，特征值和特征向量被广泛应用于数据降维、主成分分析、图像处理、物理模拟等领域。
>
>需要注意的是，特征值和特征向量通常是成对出现的，一个特征值对应一个特征向量。特征向量可以按比例缩放，因此通常会进行归一化处理，使其长度为1。
>
>对称方阵是指一个矩阵的转置与其本身相等的方阵。V=V^T

# 推荐算法

## 协同过滤算法

常用的协同过滤算法有以下几种：

1. **基于用户的协同过滤算法（User-Based Collaborative Filtering）**：根据**用户之间的相似性**以及**其他用户对物品的评分**来推荐物品。常用的相似性度量方法是余弦相似性、皮尔逊相关性或杰卡德（Jaccard）相似性。
   - 通常**杰卡德相似系数**常用在**0，1类型布尔数据集**上，皮尔逊相关系数通常用在**数值数据集**上。
2. **基于物品的协同过滤算法（Item-Based Collaborative Filtering）**：根据**物品之间的相似性**以及**用户对物品的评分**来推荐物品。常用的相似性度量方法是余弦相似性、皮尔逊相关性或杰卡德（Jaccard）相似性。
3. **基于模型的协同过滤算法（Model-Based Collaborative Filtering）**：通过**建立一个预测模型来预测用户对未评分物品的评分**。常用的模型包括协同过滤矩阵分解（Collaborative Filtering Matrix Factorization）、神经网络（Neural Networks）、关联规则挖掘（Association Rule Mining）等。
4. **基于深度学习的协同过滤算法（Deep Learning Collaborative Filtering）**：**利用深度学习模型来预测用户对未评分物品的评分**。常用的深度学习模型包括循环神经网络（Recurrent Neural Networks）、自注意力模型（Self-Attention Models）等。
5. **基于强化学习的协同过滤算法（Reinforcement Learning Collaborative Filtering）**：利用强化学习来优化推荐策略。常用的奖励函数包括点击率、购买率、平均评分等。

这些协同过滤算法各有优缺点，应根据具体的应用场景和数据特点选择合适的算法。

基于内存的协同过滤算法是一种推荐系统算法，它利用用户之间的相似性或物品之间的相似性来进行推荐。该算法主要基于用户的历史行为或物品的属性，而不需要显式地建立模型。

基于内存的协同过滤算法通常有两种类型：基于用户的协同过滤和基于物品的协同过滤。

1. 基于用户的协同过滤（User-Based Collaborative Filtering）：该方法通过计算用户之间的相似性来进行推荐。具体步骤如下：
   - 计算用户之间的相似性，常用的相似性度量方法包括余弦相似度、皮尔逊相关系数等。
   - 根据用户的相似性，找到与目标用户最相似的一组用户。
   - 根据这组相似用户的历史行为，推荐目标用户可能感兴趣的物品。

2. 基于物品的协同过滤（Item-Based Collaborative Filtering）：该方法通过计算物品之间的相似性来进行推荐。具体步骤如下：
   - 计算物品之间的相似性，常用的相似性度量方法包括余弦相似度、皮尔逊相关系数等。
   - 根据物品的相似性，找到与目标物品最相似的一组物品。
   - 根据这组相似物品的历史行为，推荐给目标用户与这些物品相关的其他物品。

基于内存的协同过滤算法简单且易于实现，但在面对大规模数据时可能会面临性能问题。此外，它也存在冷启动问题，即对于新用户或新物品，由于缺乏历史数据，无法进行准确的推荐。因此，在实际应用中，通常会结合其他推荐算法来提高推荐的准确性和覆盖率。

### ALS算法（交替最小二乘法）

ALS算法是一种**基于矩阵分解的协同过滤推荐算法**，全称为**交替最小二乘法**（Alternating Least Squares）。

传统的协同过滤推荐算法主要**基于用户-物品的评分矩阵进行计算**，**但当用户-物品的评分矩阵较为稀疏时，推荐结果的准确性会受到影响**。ALS算法通过矩阵分解技术，将用户-物品的评分矩阵分解为用户矩阵和物品矩阵，并通过迭代优化这两个矩阵来提高推荐准确性。

具体来说，ALS算法采用**最小二乘法**来优化用户和物品矩阵，**每次迭代时，先固定用户矩阵，优化物品矩阵，然后固定物品矩阵，优化用户矩阵，直到达到预设的迭代次数或收敛条件**。在优化过程中，算法通过迭代交替的方式逐步改进用户和物品矩阵，以提高推荐结果的准确性。

ALS算法已被广泛应用于推荐场景中，如电商、新闻、广告等领域的个性化推荐系统。

```python
from pyspark.ml.recommendation import ALS  
from pyspark.sql import SparkSession  
  
# 创建一个SparkSession  
spark = SparkSession.builder.appName('ALSRecommendation').getOrCreate()  
  
# 加载数据集，数据集中的每一行包含用户ID、物品ID和评分  
data = spark.read.csv('your_data.csv', header=True, inferSchema=True)  
  
# 训练ALS模型  
als = ALS(userCol="user_id", itemCol="item_id", ratingCol="rating",coldStartStrategy="drop")  
model = als.fit(data)  
  
# 计算用户之间的相似度  
userSimilarityMatrix = model.userSimilarities  
  
# 输出用户相似度矩阵  
for i in range(userSimilarityMatrix.shape[0]):  
    for j in range(userSimilarityMatrix.shape[1]):  
        print(f"{i}\t{j}\t{userSimilarityMatrix[i][j]}")  
  
# 关闭SparkSession  
spark.stop()
```



# 模型评价指标

## 均方根误差（RMSE）和均方误差（MSE）

均方根误差（Root Mean Squared Error, RMSE）是一种常用的回归模型评估指标，用于衡量模型预测值和实际观测值的差异程度。它是均方误差（Mean Squared Error，MSE）的平方根。

均方根误差

```matlab
RMSE = sqrt((1/n) Σ(yᵢ - ŷᵢ)²)
```

均方误差：

```matlab
MSE = (1/n) * Σ(yᵢ - ŷᵢ)²
```

> n是样本个数，yᵢ是第i个样本的实际观测值，ŷᵢ是第i个样本的预测值。

RMSE的值越小，表示模型的预测能力越好，与实际观测值的差异越小。

RMSE的优点是对预测误差的较大值更加敏感，因为它对误差进行了平方操作。这使得RMSE在回归问题中广泛使用，特别是当存在较大的误差值时。

### RMSE与MSE的区别

RMSE是MSE的平方根；

MSE：

- MSE计算的是预测值与实际观测值之间差异的平方的平均值；
- 单位是原始数据单位的平方；
- MSE对所有误差值都平等对待；
  - 因为在计算平均值时，每个误差值都被平等考虑，没有特别的权重分配；
  - MSE（均方误差）是真实值与预测值的差值的平方然后求和平均，是衡量预测误差的一种指标。MSE对所有误差值都平等对待。

RMSE：

- 而RMSE是对MSE进行了平方根操作，单位与原始数据的单位相同；
- RMSE对预测误差的较大值更加敏感：RMSE因为它是先对误差进行平方的累加后再开方，它其实是放大了较大误差之间的差距，这也意味RMSE在回归问题中更加关注较大的误差值；

## Silhouette分数-评估聚类质量

Silhouette分数是一种用于评估聚类质量的指标。它结合了聚类内部的紧密度和聚类之间的分离度，可以帮助我们判断聚类结果的好坏。

Silhouette分数主要用于评价样本的聚类结果与其实际标签之间的相似度。

计算Silhouette分数时，先计算样本i属于某个簇的平均距离，称为簇内距离；然后计算样本i属于其他簇的平均距离，称为簇间距离。Silhouette分数由簇间距离减去簇内距离得到。

Silhouette分数的取值范围在[-1, 1]之间，具体解释如下：

- 当Silhouette分数接近于1时，表示样本在自己的聚类中紧密地聚集在一起，并且与其他聚类之间有很好的分离度。
- 当Silhouette分数接近于0时，表示样本在聚类边界附近，可能存在重叠或者不明显的聚类结构。
- 当Silhouette分数接近于-1时，表示样本被错误地分配到了其他聚类中，聚类结果可能是不合理的。

因此，Silhouette分数越接近于1，表示聚类结果越好；越接近于-1，表示聚类结果越差。

在代码中，我们使用ClusteringEvaluator来计算聚类结果的Silhouette分数。通过比较不同聚类算法的Silhouette分数，我们可以评估它们在给定数据集上的聚类性能。

```python
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StringIndexer
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

# 假设df是包含客户数据的DataFrame，features是用于聚类的特征列名列表

# 创建特征向量
vecAssembler = VectorAssembler(inputCols=features, outputCol="features")
df_features = vecAssembler.transform(df)

# 将布尔类型特征转换为数值类型（0表示False，1表示True）
bool_features = ['bool_feature1', 'bool_feature2']
for feature in bool_features:
    indexer = StringIndexer(inputCol=feature, outputCol=feature+"_index")
    df_features = indexer.fit(df_features).transform(df_features)

# 选择需要的特征列
selected_features = features + [feature+"_index" for feature in bool_features]
df_selected_features = df_features.select(selected_features)

# 使用K-Means进行聚类
kmeans = KMeans(k=3, seed=1)
model = kmeans.fit(df_selected_features)
df_clustered = model.transform(df_selected_features)

# 计算聚类结果的Silhouette分数
evaluator = ClusteringEvaluator()
silhouette_score = evaluator.evaluate(df_clustered)

print("Silhouette score: ", silhouette_score)
```

# 特征选择技术

有些模型对人群进行分组后，很难向业务解释其结果。对于这类分组结果，可以让系统根据数据挖掘中的特征选择技术，如信息增益 IG（Information Gain）、开方检验 CHI 等，来确定这组人群应该有怎样的特征，并将其作为标签。

在面对难以理解的高维特征空间时。通过使用特征选择技术，我们可以找出对分组结果影响最大的特征，从而形成更具解释性的特征集合。

1. **信息增益（IG）**：这是一种基于决策树的特征选择方法。它衡量的是**每个特征对于分类结果的信息贡献，通过计算每个特征的熵（或者说是对概率分布的不确定性）在考虑和不考虑该特征时的差值来衡量信息增益**。
2. **卡方检验（CHI）**：这是一种基于统计学的特征选择方法。它**通过计算每个特征和分类结果之间的相关性，以及这种相关性是否显著，来决定是否选择该特征**。

这两种方法都可以帮助我们找出对分组结果影响最大的特征，但是它们各有优缺点。例如，信息增益可能偏向选择具有大量不同可能值的特征，而卡方检验可能对离散特征更为敏感。因此，在实际应用中，我们可能需要尝试多种不同的特征选择方法，以获得最合适的结果。

最后，通过将这些选择的特征组合成一个易于理解的标签，我们可以为业务提供一个清晰的指南，以理解并利用这些分组结果。这种方法尤其在面对需要人工介入的情况时非常有用，比如在面对一个需要人工解释的决策时，或者在需要提供一个易于理解的报告时。

# 聚类分析

## 聚类客户分群后，确定每一个客群的典型特征和标签

确定每一个客群的典型特征和标签可以通过以下方法：

1. 特征重要性：对于使用聚类算法得到的客群，可以使用特征重要性评估方法来确定每个客群中最具有区分性和重要性的特征。一种常用的方法是使用决策树算法，如随机森林或梯度提升树，通过评估特征在决策树中的分裂贡献来计算特征的重要性。较高重要性的特征可以被认为是该客群的典型特征。

2. 数据可视化：通过将客群的数据进行可视化，可以观察不同客群之间的特征分布和差异。常用的可视化方法包括散点图、箱线图、直方图等。通过观察不同客群在不同特征上的分布情况，可以确定每个客群的典型特征。

3. 领域知识和业务理解：结合领域知识和业务理解，可以根据对客群的理解和需求来确定每个客群的典型特征和标签。例如，在电商领域，可以根据购买行为、偏好和消费金额等特征来确定客群的典型特征和标签，如高价值客户、折扣敏感客户等。

4. 聚类算法的输出：某些聚类算法（如K-Means）会为每个客群分配一个中心点，该中心点可以被视为该客群的典型特征。中心点的特征值可以用来描述该客群的特征。

```python
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler

# 假设已经有一个DataFrame df，包含客户数据，其中features列包含了用于聚类的特征

# 创建特征向量
assembler = VectorAssembler(inputCols=["feature1", "feature2", ...], outputCol="features")
df = assembler.transform(df)

# 训练K-Means模型
kmeans = KMeans(k=3)  # 假设要将客户分为3个群组
model = kmeans.fit(df)

# 获取每个客群的中心点
centers = model.clusterCenters()

# 获取每个客群的标签
labels = model.transform(df).select("prediction").rdd.flatMap(lambda x: x).collect()

# 打印每个客群的典型特征和标签
for i in range(len(centers)):
    print("Cluster", i+1)
    print("Center:", centers[i])
    print("Label:", labels[i])
    print()
```

在上述代码中，首先使用VectorAssembler将特征列合并为一个特征向量。然后，使用KMeans算法训练模型，并通过clusterCenters()方法获取每个客群的中心点。最后，使用transform()方法获取每个客户的预测标签，并打印出每个客群的典型特征和标签。

请注意，代码中的feature1、feature2等应替换为实际的特征列名，k的值可以根据实际需求进行调整。

## 找出对聚类客户分群影响最大的特征集合

要找出对聚类客户分群影响最大的特征集合，可以使用特征重要性评估方法。以下是一种基于PySpark的示例代码，演示如何计算特征重要性并找出对聚类客户分群影响最大的特征集合：

```python
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler

# 假设已经有一个DataFrame df，包含客户数据，其中features列包含了用于聚类的特征

# 创建特征向量
assembler = VectorAssembler(inputCols=["feature1", "feature2", ...], outputCol="features")
df = assembler.transform(df)

# 训练K-Means模型
kmeans = KMeans(k=3)  # 假设要将客户分为3个群组
model = kmeans.fit(df)

# 获取每个特征的重要性
feature_importances = model.featureImportances

# 将特征重要性与特征列名对应起来
feature_importances_dict = dict(zip(assembler.getInputCols(), feature_importances))

# 按照特征重要性降序排序
sorted_features = sorted(feature_importances_dict.items(), key=lambda x: x[1], reverse=True)

# 打印对聚类客户分群影响最大的特征集合
top_features = [feature for feature, importance in sorted_features[:5]]  # 假设选择前5个特征
print("Top features:", top_features)
```

在上述代码中，首先使用VectorAssembler将特征列合并为一个特征向量。然后，使用KMeans算法训练模型，并通过featureImportances属性获取每个特征的重要性。将特征重要性与特征列名对应起来，并按照重要性降序排序。最后，选择前几个重要性最高的特征作为对聚类客户分群影响最大的特征集合。

请注意，代码中的feature1、feature2等应替换为实际的特征列名，k的值可以根据实际需求进行调整。在示例代码中，选择了前5个重要性最高的特征，你可以根据实际情况调整这个数量。

