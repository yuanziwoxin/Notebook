以下是一些常用的 PySpark 数据分析、数据预处理和特征工程的代码示例：

1. 数据读取和基本操作：
from pyspark.sql import SparkSession

# 创建 SparkSession
spark = SparkSession.builder.getOrCreate()

# 读取 CSV 文件
df = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)

# 显示 DataFrame 的前几行
df.show()

# 查看 DataFrame 的列名
df.columns

# 查看 DataFrame 的统计摘要信息
df.describe().show()

2. 数据清洗和处理缺失值：

# 删除包含缺失值的行
df = df.dropna()

# 填充缺失值
df = df.fillna(0)  # 使用 0 填充缺失值

# 替换特定值
df = df.replace("unknown", "NA")  # 将 "unknown" 替换为 "NA"

3. 特征选择和转换：

from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder

# 合并多个特征列为一个向量列
assembler = VectorAssembler(inputCols=["col1", "col2", "col3"], outputCol="features")
df = assembler.transform(df)

# 对分类特征进行索引编码
indexer = StringIndexer(inputCol="category", outputCol="category_index")
df = indexer.fit(df).transform(df)

# 对索引编码后的分类特征进行独热编码
encoder = OneHotEncoder(inputCol="category_index", outputCol="category_encoded")
df = encoder.transform(df)

4. 数据标准化和归一化：

from pyspark.ml.feature import StandardScaler, MinMaxScaler

# 标准化特征列
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(df)
df = scaler_model.transform(df)

# 归一化特征列
min_max_scaler = MinMaxScaler(inputCol="features", outputCol="normalized_features")
df = min_max_scaler.fit(df).transform(df)	

这些代码示例涵盖了常见的数据分析、数据预处理和特征工程任务。根据你的具体需求，你可以根据这些示例 


5. 特征选择和降维：

from pyspark.ml.feature import PCA, ChiSqSelector

# 使用主成分分析进行特征降维
pca = PCA(k=3, inputCol="features", outputCol="pca_features")
pca_model = pca.fit(df)
df = pca_model.transform(df)

# 使用卡方检验选择相关特征
selector = ChiSqSelector(numTopFeatures=5, featuresCol="features", outputCol="selected_features")
selector_model = selector.fit(df)
df = selector_model.transform(df)

6. 数据分割和抽样：

from pyspark.sql.functions import rand

# 随机分割数据集为训练集和测试集
train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)

# 对数据进行抽样
sample_df = df.sample(fraction=0.1, seed=42)

7. 特征重要性评估：

from pyspark.ml.feature import VectorIndexer
from pyspark.ml.regression import RandomForestRegressor

# 对特征进行索引编码
indexer = VectorIndexer(inputCol="features", outputCol="indexed_features", maxCategories=10)
indexed_df = indexer.fit(df).transform(df)

# 训练随机森林回归模型
rf = RandomForestRegressor(featuresCol="indexed_features", labelCol="label")
rf_model = rf.fit(indexed_df)

# 查看特征重要性
feature_importances = rf_model.featureImportances

这些示例代码涵盖了常见的 PySpark 数据分析、数据预处理和特征工程任务。根据你的具体需求，你可以根据这些示例进行定制和扩展

以下是使用PySpark编写数据清洗和数据过滤的示例代码：

1. 数据清洗：删除包含缺失值的行

df = df.dropna()

2. 数据清洗：填充缺失值

df = df.fillna(0)  # 使用 0 填充缺失值

3. 数据过滤：根据条件过滤行

df = df.filter(df['age'] > 18)  # 过滤年龄大于 18 的行

4. 数据过滤：根据列值进行筛选

df = df.filter(df['category'].isin(['A', 'B', 'C']))  # 筛选 category 列值为 A、B 或 C 的行



以下是使用PySpark进行缺失值补充的多种方法的示例代码：

1. 使用平均值填充缺失值：
from pyspark.ml.feature import Imputer

# 创建 Imputer 对象，设置输入列和输出列
imputer = Imputer(inputCols=["col1", "col2", "col3"], outputCols=["col1_imputed", "col2_imputed", "col3_imputed"])

# 使用平均值填充缺失值
imputer_model = imputer.setStrategy("mean").fit(df)
df_imputed = imputer_model.transform(df)

2. 使用中位数填充缺失值：
from pyspark.ml.feature import Imputer

# 创建 Imputer 对象，设置输入列和输出列
imputer = Imputer(inputCols=["col1", "col2", "col3"], outputCols=["col1_imputed", "col2_imputed", "col3_imputed"])

# 使用中位数填充缺失值
imputer_model = imputer.setStrategy("median").fit(df)
df_imputed = imputer_model.transform(df)

3. 使用众数填充缺失值：
from pyspark.ml.feature import Imputer

# 创建 Imputer 对象，设置输入列和输出列
imputer = Imputer(inputCols=["col1", "col2", "col3"], outputCols=["col1_imputed", "col2_imputed", "col3_imputed"])

# 使用众数填充缺失值
imputer_model = imputer.setStrategy("mode").fit(df)
df_imputed = imputer_model.transform(df)

4. 使用指定值填充缺失值：
from pyspark.ml.feature import Imputer

# 创建 Imputer 对象，设置输入列和输出列
imputer = Imputer(inputCols=["col1", "col2", "col3"], outputCols=["col1_imputed", "col2_imputed", "col3_imputed"])

# 使用指定值填充缺失值
imputer_model = imputer.setMissingValue(0).fit(df)
df_imputed = imputer_model.transform(df)


以下是使用PySpark进行数据过滤和筛选的多种具体方法的示例代码：

1. 使用filter方法根据条件过滤行：
df_filtered = df.filter(df['age'] > 18)  # 过滤年龄大于 18 的行

2. 使用where方法根据条件过滤行：
df_filtered = df.where(df['age'] > 18)  # 过滤年龄大于 18 的行

3. 使用select方法选择特定列：
df_selected = df.select('name', 'age')  # 选择 name 和 age 列

4. 使用drop方法删除指定列：
df_dropped = df.drop('age')  # 删除 age 列

5. 使用groupBy和agg方法进行分组和聚合操作：
df_grouped = df.groupBy('category').agg({'price': 'mean', 'quantity': 'sum'})  # 按 category 分组，计算 price 的平均值和 quantity 的总和

6. 使用orderBy方法按照指定列进行排序：
df_sorted = df.orderBy('age', ascending=False)  # 按照 age 列降序排序


以下是使用PySpark进行特征处理的多种方法的示例代码：

1. 使用VectorAssembler将多个特征列合并为一个特征向量：
from pyspark.ml.feature import VectorAssembler

# 假设'feature1'和'feature2'是要合并的特征列
assembler = VectorAssembler(inputCols=['feature1', 'feature2'], outputCol='features')
df_assembled = assembler.transform(df)

2. 使用StringIndexer将分类特征转换为数值特征：
from pyspark.ml.feature import StringIndexer

# 假设'category'是要转换的分类特征列
indexer = StringIndexer(inputCol='category', outputCol='category_index')
df_indexed = indexer.fit(df).transform(df)

3. 使用OneHotEncoder将数值特征转换为独热编码：
from pyspark.ml.feature import OneHotEncoder

# 假设'category_index'是要转换的数值特征列
encoder = OneHotEncoder(inputCol='category_index', outputCol='category_encoded')
df_encoded = encoder.transform(df_indexed)

4. 使用StandardScaler对数值特征进行标准化：
from pyspark.ml.feature import StandardScaler

# 假设'feature1'是要标准化的数值特征列
scaler = StandardScaler(inputCol='feature1', outputCol='feature1_scaled')
scaler_model = scaler.fit(df)
df_scaled = scaler_model.transform(df)

5. 使用PCA进行主成分分析降维：
from pyspark.ml.feature import PCA

# 假设'features'是要进行主成分分析的特征列
pca = PCA(k=2, inputCol='features', outputCol='pca_features')
pca_model = pca.fit(df)
df_pca = pca_model.transform(df)


以下是使用PySpark绘制各种图表的具体方法的示例代码：

1. 绘制柱状图：
import pyspark.pandas as ps

# 假设'df'是你的DataFrame，'column'是要绘制柱状图的列名
df[column].to_pandas().plot.bar()
2. 绘制折线图：
import pyspark.pandas as ps

# 假设'df'是你的DataFrame，'x_column'和'y_column'是要绘制折线图的列名
df.plot.line(x=x_column, y=y_column)

3. 绘制散点图：
import pyspark.pandas as ps

# 假设'df'是你的DataFrame，'x_column'和'y_column'是要绘制散点图的列名
df.plot.scatter(x=x_column, y=y_column)

4. 绘制饼图：
import pyspark.pandas as ps

# 假设'df'是你的DataFrame，'column'是要绘制饼图的列名
df[column].to_pandas().plot.pie()

5. 绘制直方图：
import pyspark.pandas as ps

# 假设'df'是你的DataFrame，'column'是要绘制直方图的列名
df[column].to_pandas().plot.hist()


这些示例代码演示了使用PySpark绘制各种图表的具体方法。请注意，这些方法使用了pyspark.pandas库，它提供了更多的绘图功能。根据你的具体需求，你可以选择适合你的方法进行图表绘制。 


